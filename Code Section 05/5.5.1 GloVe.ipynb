{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحميل المكتبات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة المسافة لاستخدامها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist1(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "def dist2(a, b):\n",
    "    return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "dist, metric = dist2, 'cosine'\n",
    "# dist, metric = dist1, 'euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة لتنفيذ عملية الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"%s not in dictionary\" % w)\n",
    "            return\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    best_word = ''\n",
    "    for word, v1 in iteritems(word2vec):\n",
    "        if word not in (w1, w2, w3):\n",
    "            d = dist(v0, v1)\n",
    "            if d < min_dist:\n",
    "                min_dist = d\n",
    "                best_word = word\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نفس الدالة السابقة بطريقة اسرع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"%s not in dictionary\" % w)\n",
    "            return\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    distances = pairwise_distances(v0.reshape(1, D),\n",
    "                                   embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[:4]\n",
    "    for idx in idxs:\n",
    "        word = idx2word[idx]\n",
    "        if word not in (w1, w2, w3): \n",
    "            best_word = word\n",
    "            break\n",
    "\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمل دالة لاختيار الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(w, n=5):\n",
    "    if w not in word2vec:\n",
    "        print(\"%s not in dictionary:\" % w)\n",
    "        return\n",
    "\n",
    "    v = word2vec[w]\n",
    "    distances = pairwise_distances(v.reshape(1, D),\n",
    "                                   embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[1:n+1]\n",
    "    print(\"neighbors of: %s\" % w)\n",
    "    for idx in idxs:\n",
    "        print(\"\\t%s\" % idx2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قراءة البيانات "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "\n",
    "with open(r'E:\\Machine Learning\\NLP\\0 Data\\GloVe English\\glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الاطلاع علي البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word the   ,   with Vecs     [ 0.418    0.24968 -0.41242  0.1217   0.34527]\n",
      "word ,   ,   with Vecs     [ 0.013441  0.23682  -0.16899   0.40951   0.63812 ]\n",
      "word .   ,   with Vecs     [ 0.15164  0.30177 -0.16763  0.17684  0.31719]\n",
      "word of   ,   with Vecs     [ 0.70853  0.57088 -0.4716   0.18048  0.54449]\n",
      "word to   ,   with Vecs     [ 0.68047  -0.039263  0.30186  -0.17792   0.42962 ]\n",
      "word and   ,   with Vecs     [ 0.26818   0.14346  -0.27877   0.016257  0.11384 ]\n",
      "word in   ,   with Vecs     [ 0.33042   0.24995  -0.60874   0.10923   0.036372]\n",
      "word a   ,   with Vecs     [ 0.21705  0.46515 -0.46757  0.10082  1.0135 ]\n",
      "word \"   ,   with Vecs     [ 0.25769  0.45629 -0.76974 -0.37679  0.59272]\n",
      "word 's   ,   with Vecs     [ 0.23727  0.40478 -0.20547  0.58805  0.65533]\n",
      "word for   ,   with Vecs     [ 0.15272   0.36181  -0.22168   0.066051  0.13029 ]\n",
      "word -   ,   with Vecs     [-0.16768  1.2151   0.49515  0.26836 -0.4585 ]\n",
      "word that   ,   with Vecs     [ 0.88387  -0.14199   0.13566   0.098682  0.51218 ]\n",
      "word on   ,   with Vecs     [ 0.30045   0.25006  -0.16692   0.1923    0.026921]\n",
      "word is   ,   with Vecs     [ 0.6185   0.64254 -0.46552  0.3757   0.74838]\n",
      "word was   ,   with Vecs     [ 0.086888 -0.19416  -0.24267  -0.33391   0.56731 ]\n",
      "word said   ,   with Vecs     [ 0.38973 -0.2121   0.51837  0.80136  1.0336 ]\n",
      "word with   ,   with Vecs     [ 0.25616  0.43694 -0.11889  0.20345  0.41959]\n",
      "word he   ,   with Vecs     [-0.20092  -0.060271 -0.61766  -0.8444    0.5781  ]\n",
      "word as   ,   with Vecs     [ 0.20782  0.12713 -0.30188 -0.23125  0.30175]\n",
      "word it   ,   with Vecs     [ 0.61183  -0.22072  -0.10898  -0.052967  0.50804 ]\n",
      "word by   ,   with Vecs     [ 0.35215 -0.35603  0.25708 -0.10611 -0.20718]\n",
      "word at   ,   with Vecs     [ 0.27724   0.88469  -0.26247   0.084104  0.40813 ]\n",
      "word (   ,   with Vecs     [-0.24978  1.0476   0.21602  0.23278  0.12371]\n",
      "word )   ,   with Vecs     [-0.28314    1.0028     0.14746    0.22262    0.0070985]\n",
      "word from   ,   with Vecs     [ 0.41037   0.11342   0.051524 -0.53833  -0.12913 ]\n",
      "word his   ,   with Vecs     [-0.033537  0.47537  -0.68746  -0.72661   0.84028 ]\n",
      "word ''   ,   with Vecs     [ 0.0028594  0.19457   -0.19449   -0.037583   0.9634   ]\n",
      "word ``   ,   with Vecs     [ 0.12817  0.15858 -0.38843 -0.39108  0.68366]\n",
      "word an   ,   with Vecs     [ 0.36143   0.58615  -0.23718   0.079656  0.80192 ]\n",
      "word be   ,   with Vecs     [ 0.91102 -0.22872  0.2077  -0.20237  0.50697]\n",
      "word has   ,   with Vecs     [0.54822  0.038847 0.10127  0.31319  0.095487]\n",
      "word are   ,   with Vecs     [ 0.96193   0.012516  0.21733  -0.06539   0.26843 ]\n",
      "word have   ,   with Vecs     [ 0.94911   -0.34968    0.48125   -0.19306   -0.0088384]\n",
      "word but   ,   with Vecs     [ 0.35934  -0.2657   -0.046477 -0.2496    0.54676 ]\n",
      "word were   ,   with Vecs     [ 0.73363  -0.74815   0.45913  -0.56041   0.091855]\n",
      "word not   ,   with Vecs     [ 0.55025   -0.24942   -0.0009386 -0.264      0.5932   ]\n",
      "word this   ,   with Vecs     [ 0.53074  0.40117 -0.40785  0.15444  0.47782]\n",
      "word who   ,   with Vecs     [-0.19461  -0.051277  0.26445  -0.57399   1.0236  ]\n",
      "word they   ,   with Vecs     [ 0.70835 -0.57361  0.15375 -0.63335  0.46879]\n",
      "word had   ,   with Vecs     [ 0.60348 -0.52096  0.40851 -0.37217  0.36978]\n",
      "word i   ,   with Vecs     [ 0.11891   0.15255  -0.082073 -0.74144   0.75917 ]\n",
      "word which   ,   with Vecs     [ 0.90558   0.054033 -0.024091  0.08111   0.08645 ]\n",
      "word will   ,   with Vecs     [0.81544 0.30171 0.5472  0.46581 0.28531]\n",
      "word their   ,   with Vecs     [ 0.41519  0.13167 -0.0569  -0.56765  0.49924]\n",
      "word :   ,   with Vecs     [-0.17587  1.3508  -0.18159  0.45197  0.37554]\n",
      "word or   ,   with Vecs     [ 0.26358   0.18747   0.044394 -0.19119   0.45455 ]\n",
      "word its   ,   with Vecs     [ 0.76719  0.1239  -0.11119  0.13355  0.18356]\n",
      "word one   ,   with Vecs     [0.31474 0.41662 0.1348  0.15854 0.88812]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b in word2vec.items() : \n",
    "    i+=1\n",
    "    if i==50 : break\n",
    "    print(f'word {a}   ,   with Vecs     {b[:5]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.418     ,  0.24968   , -0.41242   ,  0.1217    ,  0.34527   ,\n",
       "       -0.044457  , -0.49688   , -0.17862   , -0.00066023, -0.6566    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمليات الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = july - june\n",
      "miami - florida = houston - texas\n",
      "einstein - scientist = matisse - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = friend - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = queen - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "cairo - egypt = damascus - syria\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')\n",
    "find_analogies('france', 'paris', 'london')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "\n",
    "find_analogies('cairo','egypt','syria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tprince\n",
      "\tqueen\n",
      "\tii\n",
      "\temperor\n",
      "\tson\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tbelgium\n",
      "\tparis\n",
      "\tspain\n",
      "\tnetherlands\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\tchina\n",
      "\tkorea\n",
      "\ttokyo\n",
      "\ttaiwan\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysics\n",
      "\theisenberg\n",
      "\tfreud\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\ther\n",
      "\tboy\n",
      "neighbors of: nephew\n",
      "\tcousin\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tson\n",
      "\tuncle\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\taugust\n",
      "\tseptember\n",
      "neighbors of: rome\n",
      "\tnaples\n",
      "\tvenice\n",
      "\titaly\n",
      "\tturin\n",
      "\tpope\n",
      "neighbors of: bolt\n",
      "\tusain\n",
      "\tjavelin\n",
      "\tkitajima\n",
      "\thammer\n",
      "\trod\n",
      "neighbors of: bus\n",
      "\ttrain\n",
      "\tbuses\n",
      "\tpassenger\n",
      "\tcommuter\n",
      "\ttrains\n",
      "neighbors of: hello\n",
      "\tgoodbye\n",
      "\they\n",
      "\t!\n",
      "\tkiss\n",
      "\twow\n",
      "neighbors of: go\n",
      "\tgoing\n",
      "\tcome\n",
      "\tget\n",
      "\t'll\n",
      "\ttake\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king')\n",
    "nearest_neighbors('france')\n",
    "nearest_neighbors('japan')\n",
    "nearest_neighbors('einstein')\n",
    "nearest_neighbors('woman')\n",
    "nearest_neighbors('nephew')\n",
    "nearest_neighbors('february')\n",
    "nearest_neighbors('rome')\n",
    "\n",
    "nearest_neighbors('bolt')\n",
    "nearest_neighbors('bus')\n",
    "nearest_neighbors('hello')\n",
    "nearest_neighbors('go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "التجربة علي الملف الأكبر ذو 300 قيمة للتضمين"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "\n",
    "with open(r'E:\\Machine Learning\\NLP\\0 Data\\GloVe English\\glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الاطلاع علي البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word the   ,   with Vecs     [ 0.04656    0.21318   -0.0074364 -0.45854   -0.035639 ]\n",
      "word ,   ,   with Vecs     [-0.25539  -0.25723   0.13169  -0.042688  0.21817 ]\n",
      "word .   ,   with Vecs     [-0.12559   0.01363   0.10306  -0.10123   0.098128]\n",
      "word of   ,   with Vecs     [-0.076947 -0.021211  0.21271  -0.72232  -0.13988 ]\n",
      "word to   ,   with Vecs     [-0.25756  -0.057132 -0.6719   -0.38082  -0.36421 ]\n",
      "word and   ,   with Vecs     [ 0.038466 -0.039792  0.082747 -0.38923  -0.21431 ]\n",
      "word in   ,   with Vecs     [-0.44399  0.12817 -0.25247 -0.18582 -0.16614]\n",
      "word a   ,   with Vecs     [-0.29712   0.094049 -0.096662 -0.344    -0.18483 ]\n",
      "word \"   ,   with Vecs     [ 0.6947    0.22184   0.10526   0.012382 -0.2558  ]\n",
      "word 's   ,   with Vecs     [-0.001272  0.36514  -0.077363 -0.26559   0.17987 ]\n",
      "word for   ,   with Vecs     [-0.24132   0.12063   0.1919   -0.26692   0.061076]\n",
      "word -   ,   with Vecs     [ 0.040598 -0.028364  0.25009  -0.26526   0.43197 ]\n",
      "word that   ,   with Vecs     [-0.18256  0.49851 -0.1639  -0.17443 -0.16382]\n",
      "word on   ,   with Vecs     [ 0.12702  0.21062 -0.18983  0.17677 -0.11784]\n",
      "word is   ,   with Vecs     [-0.1749   0.22956  0.24924 -0.20512 -0.12294]\n",
      "word was   ,   with Vecs     [ 0.065573  0.022011 -0.13182  -0.2133   -0.045275]\n",
      "word said   ,   with Vecs     [-0.24135   0.15132   0.016839  0.15962   0.20608 ]\n",
      "word with   ,   with Vecs     [-0.50425  0.31073  0.1428  -0.49815 -0.48062]\n",
      "word he   ,   with Vecs     [-0.52057   0.49562   0.032912 -0.060807 -0.099912]\n",
      "word as   ,   with Vecs     [-0.056826  0.23863   0.44515  -0.014863  0.1713  ]\n",
      "word it   ,   with Vecs     [ 0.033284 -0.040754 -0.048377  0.12017  -0.13915 ]\n",
      "word by   ,   with Vecs     [ 0.073515  0.25736   0.2359    0.19299  -0.29611 ]\n",
      "word at   ,   with Vecs     [-0.36898   0.89703   0.34799  -0.062829  0.18046 ]\n",
      "word (   ,   with Vecs     [-0.1686  -0.1958  -0.20248  0.20137  0.28469]\n",
      "word )   ,   with Vecs     [-0.13877 -0.24946 -0.13896  0.105    0.2721 ]\n",
      "word from   ,   with Vecs     [-0.18559  -0.41772  -0.019296 -0.065099 -0.41371 ]\n",
      "word his   ,   with Vecs     [-0.2213    0.47139  -0.16658  -0.26128   0.040058]\n",
      "word ''   ,   with Vecs     [-0.32597  -0.3418    0.33624   0.017565  0.2899  ]\n",
      "word ``   ,   with Vecs     [-0.22276 -0.30464  0.17728 -0.10514  0.16184]\n",
      "word an   ,   with Vecs     [-0.3206    0.43316  -0.086867  0.3012   -0.093775]\n",
      "word be   ,   with Vecs     [-0.33848   0.42841  -0.10284  -0.22054   0.051708]\n",
      "word has   ,   with Vecs     [-0.2106    0.22677   0.063678  0.088348  0.2588  ]\n",
      "word are   ,   with Vecs     [-0.23589   0.3831    0.10834  -0.063118  0.031596]\n",
      "word have   ,   with Vecs     [-0.19563  0.48127 -0.2467   0.12751  0.30997]\n",
      "word but   ,   with Vecs     [-0.0093601  0.22789   -0.10275    0.0010893  0.21235  ]\n",
      "word were   ,   with Vecs     [-0.16909  0.15969 -0.18385 -0.14528 -0.17565]\n",
      "word not   ,   with Vecs     [ 0.0083903  0.28769   -0.23466   -0.26343   -0.05723  ]\n",
      "word this   ,   with Vecs     [-0.20437   0.16431   0.041794 -0.13708  -0.29779 ]\n",
      "word who   ,   with Vecs     [-0.52869   0.33011   0.18584   0.076365  0.23219 ]\n",
      "word they   ,   with Vecs     [-0.26625   0.47964  -0.38818   0.039296 -0.020338]\n",
      "word had   ,   with Vecs     [-0.32601     0.25829    -0.391      -0.13565    -0.00074383]\n",
      "word i   ,   with Vecs     [-0.13292   0.16985  -0.1436   -0.088722  0.07951 ]\n",
      "word which   ,   with Vecs     [-0.22232   0.23856  -0.048047 -0.25248  -0.08231 ]\n",
      "word will   ,   with Vecs     [-0.34572   0.28478  -0.48478  -0.34118   0.054563]\n",
      "word their   ,   with Vecs     [-0.070601  0.54909  -0.25536  -0.16544  -0.034464]\n",
      "word :   ,   with Vecs     [ 0.096348 -0.21428   0.14032  -0.054121  0.70784 ]\n",
      "word or   ,   with Vecs     [-0.26234   0.12633  -0.084185 -0.077551 -0.08509 ]\n",
      "word its   ,   with Vecs     [ 0.32509   0.01838  -0.071966 -0.42953  -0.14065 ]\n",
      "word one   ,   with Vecs     [-0.36756   0.395    -0.27034  -0.14817  -0.026378]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b in word2vec.items() : \n",
    "    i+=1\n",
    "    if i==50 : break\n",
    "    print(f'word {a}   ,   with Vecs     {b[:5]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04656  ,  0.21318  , -0.0074364, -0.45854  , -0.035639 ,\n",
       "        0.23643  , -0.28836  ,  0.21521  , -0.13486  , -1.6413   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "عمليات الطرح"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = april - june\n",
      "miami - florida = dallas - texas\n",
      "einstein - scientist = picasso - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = brother - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = prince - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "cairo - egypt = damascus - syria\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')\n",
    "find_analogies('france', 'paris', 'london')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "\n",
    "find_analogies('cairo','egypt','syria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الكلمات الاقرب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tqueen\n",
      "\tprince\n",
      "\tmonarch\n",
      "\tkingdom\n",
      "\tthrone\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tparis\n",
      "\tbelgium\n",
      "\tspain\n",
      "\titaly\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\ttokyo\n",
      "\tkorea\n",
      "\tchina\n",
      "\tasia\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysicists\n",
      "\theisenberg\n",
      "\tsigmund\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\tshe\n",
      "\ther\n",
      "neighbors of: nephew\n",
      "\tbrother\n",
      "\tcousin\n",
      "\tgrandson\n",
      "\tuncle\n",
      "\tson\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\tnovember\n",
      "\tapril\n",
      "neighbors of: rome\n",
      "\titaly\n",
      "\tnaples\n",
      "\tturin\n",
      "\tvenice\n",
      "\troman\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king')\n",
    "nearest_neighbors('france')\n",
    "nearest_neighbors('japan')\n",
    "nearest_neighbors('einstein')\n",
    "nearest_neighbors('woman')\n",
    "nearest_neighbors('nephew')\n",
    "nearest_neighbors('february')\n",
    "nearest_neighbors('rome')\n",
    "\n",
    "nearest_neighbors('bolt')\n",
    "nearest_neighbors('bus')\n",
    "nearest_neighbors('hello')\n",
    "nearest_neighbors('go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
